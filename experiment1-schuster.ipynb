{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bc9615-b7a0-498f-908d-891490f7b66e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Experiment: Familiarizing with Tools, Loading Data, and Basic Analysis of Data\n",
    "Done by Marek Schuster (ms2228) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6964f7",
   "metadata": {},
   "source": [
    "## Execution details\n",
    "All of this work was done using a M1 MBP with 16GB RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a80d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to measure the runtime of our code, so we have to import time.\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# use pyspark, create new empty context in a new session.\n",
    "spark = SparkSession.builder.appName(\"projectOne\").getOrCreate()\n",
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146010f",
   "metadata": {},
   "source": [
    "## Exercise 1.2\n",
    "\n",
    "#### a) *userRatingsRDD*: create a pair RDD from *user_libraries.txt* using the user hash as the key and the liked paper(s) (*paper id*) as the value(s)\n",
    "\n",
    "To create this RDD we will add users_libraries.txt to the context.  \n",
    "A line equals a user and the papers he liked in the format:\n",
    "\n",
    "{user-hash};{paper-id1},{paper-id2},...\n",
    "\n",
    "With varying numbers of papers.  \n",
    "So we split by the semicolon (separate user and the papers) giving us the user and split the paper-string by the comma, giving us a list of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b57372cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "userRatingsRDD_unflat = sc.textFile('users_libraries.txt')\n",
    "userRatingsRDD_unflat = userRatingsRDD_unflat.map(lambda s: (s.split(';')[0], [int(x) for x in s.split(';')[1].split(',')]))\n",
    "userRatingsRDD = userRatingsRDD_unflat.flatMapValues(lambda x: x)\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d73e96",
   "metadata": {},
   "source": [
    "#### b) *paperTermsRDD* : create a pair RDD from *papers.csv* using the *paper_id* as the key and the words contained in the abstract as the value(s).\n",
    "\n",
    "A line of a paper looks as follows:  \n",
    "{paper-id:integer},{type:string},{journal:string},{book-title:string},{series:string},{publisher:string},{pages:string},{volume},{number},{year},{month:string},{postedat:timestamp},{address:string},{title:string},{abstract:string}\n",
    "\n",
    "Since we can not load it into any other format we have to manipulate the lines in RDD a bit:  \n",
    "paper-id (integer) is on the left until the first colon is reached.  \n",
    "abstract (string) is on the rightmost field of the csv. Due to the encapsulating of strings containing commas, we have to differentiate: If abstract contains a colon, it's encapsulated in double-quotes. If not, it's not. Also we reomve punctuations directly attached to a word because those could mess with the following wordcount excercise (eg. \"text,\" would be counted as a separate word from just \"text\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9ab08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "def tidyText(txt):\n",
    "    # remove ,\"\" because it can fuck up splitting by ,\" if neccessary\n",
    "    abstract = txt.replace(\",\\\"\\\"\", \"\")\n",
    "    abstract = abstract.split(',\\\"')[-1] if txt[-1] == '\\\"' else abstract.split(',')[-1]\n",
    "    # remove other symbols we do not need to care about\n",
    "    # @@@ Replace with a filter!!!\n",
    "    abstract = abstract.replace(', ', ' ')\n",
    "    abstract = abstract.replace('. ', ' ')\n",
    "    abstract = abstract.replace('\\\"', '')\n",
    "    #abstract = abstract.replace('-', '')\n",
    "    #abstract = abstract.replace('{', '')\n",
    "    #abstract = abstract.replace('}', '')\n",
    "    #abstract = abstract.replace('\\\"', '')\n",
    "\n",
    "    abstract = abstract.split(' ')\n",
    "    return abstract\n",
    "\n",
    "paperTermsRDD_unflat = sc.textFile('papers.csv').map(lambda s: (int(s.split(',')[0]),tidyText(s)))\n",
    "paperTermsRDD = paperTermsRDD_unflat.flatMapValues(lambda x: x)\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f6c92",
   "metadata": {},
   "source": [
    "## Exercise 1.3\n",
    "#### Create a script that computes for each user the top-10 most frequent words appearing in the papers she likes. Exclude the stop words listed in stopwords en.txt. Store the results into a file which contains in each line the user hash and the list of her retrieved words sorted by frequency (top 1 is the most frequent): user hash, top 1 word, top 2 word, top 3 word,..., top 10 word\n",
    "\n",
    "To accomplish this goal we first have to flatten our key-value pairs ((K, Seq[V]) -> (K, V1), (K,V2), ...) to comfortably process the data (we already did this in the above code cell). Next we prepare the stopwords as a broadcast variable for later use. After preparation we swap keys and values in the userRatingsRDD so paper_id is now the key, this allows us to later join both RDDs on the paper_id. Before the join we also filter out all stopwords and empty words (as in: \"\").\n",
    "\n",
    "After joining we remap our new RDD so it is usable for performing a \"wordcount\" by reducing, similar to the example from the spark lecture. Atlast we sort the RDD by word occurrences descending and then remap a last time to remove the numbers as we don't need them anymore. No we just iterate over our final RDD and print each user with their respective top 10 papers to a textfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d000da91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4\n"
     ]
    }
   ],
   "source": [
    "# Create broadcast of stopwords\n",
    "# Read stopword-file and flatten it.\n",
    "stopwordsRDD = sc.textFile('stopwords_en.txt').flatMap(lambda line: line.split(\" \"))\n",
    "# broadcast the stopword-rdd.\n",
    "stopwordsBC = sc.broadcast(stopwordsRDD.collect())\n",
    "\n",
    "# Create Swapped userRatingsRDD (paper, user) so we can me8rge it:\n",
    "userRatingsRDDs = userRatingsRDD.map(lambda s: (s[1], s[0]))\n",
    "\n",
    "# remove all emtpy words and words that are part of stopwords.\n",
    "paperTermsRDDf = paperTermsRDD.filter(lambda s: s[1] and s[1] not in stopwordsBC.value)\n",
    "\n",
    "# Join them together based on paper id using only words that are not blacklisted or empty:\n",
    "joined = userRatingsRDDs.join(paperTermsRDDf)\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4a2798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4\n"
     ]
    }
   ],
   "source": [
    "# restructure to ((user, word), 1) to do a simple word-count\n",
    "wcable = joined.map(lambda s: ((s[1][0],s[1][1]), 1))\n",
    "\n",
    "# word count\n",
    "wcable = wcable.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e974d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647.3\n"
     ]
    }
   ],
   "source": [
    "# Sort table by occurence of words descending.\n",
    "wcable_desc = wcable.sortBy(lambda s: -s[1])\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34ff529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647.4\n"
     ]
    }
   ],
   "source": [
    "# restructure again to (user, (word, number))\n",
    "wcable_desc2 = wcable_desc.map(lambda s: (s[0][0], (s[0][1], s[1])))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d8468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647.4\n"
     ]
    }
   ],
   "source": [
    "# Group entries by user, so we can get top 10 easily\n",
    "wcable_desc3 = wcable_desc2.groupByKey()\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a336e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991.0\n"
     ]
    }
   ],
   "source": [
    "#Iterate over the RDD writing user_id and their top 10 papers as lines into a textfile\n",
    "with open(\"wordcountRDD.txt\", \"w\") as file:\n",
    "    i = -1\n",
    "    for x in wcable_desc3.collect():\n",
    "        if(i >= 0):\n",
    "            file.write(\"\\n\");\n",
    "        file.write(str(x[0]))\n",
    "        i = 0\n",
    "        for y in x[1]:\n",
    "            if i <= 9:\n",
    "                file.write(\",\" + str(y[0]))\n",
    "            else:\n",
    "                continue;\n",
    "            i += 1\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1096ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52381857\n",
      "20984374\n"
     ]
    }
   ],
   "source": [
    "print(joined.count())\n",
    "print(wcable.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8ed2d",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "#### a) Number of (distinct) user, number of (distinct) items, and number of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff537af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 28416\n",
      "Number of papers: 172079\n",
      "Number of ratings: 828481\n"
     ]
    }
   ],
   "source": [
    "# Calculate users based on the unique keys (unique key = unique user)\n",
    "# Grouping by key to make sure no user is counted twice.\n",
    "print(\"Number of users:\", userRatingsRDD_unflat.groupByKey().count())\n",
    "\n",
    "# Calculate papers based on the unique keys (unique key = unique user)\n",
    "# Grouping by key to make sure no paper is counted twice.\n",
    "print(\"Number of papers:\", paperTermsRDD_unflat.groupByKey().count())\n",
    "\n",
    "# Number of user ratings equals length is userRatingsRDD (each pair is one rating)\n",
    "print(\"Number of ratings:\", userRatingsRDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5b4fe",
   "metadata": {},
   "source": [
    "#### b) Min number of ratings a user has given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57190afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. number of ratings by user: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a pair of (user_id, 1), execute a word count, restructure to flat list of only count of ratings of users\n",
    "numberOfRaitingsRDD = userRatingsRDD.map(lambda pair: (pair[1], 1)).reduceByKey(lambda x, y: x+y).map(lambda x: x[1])\n",
    "\n",
    "# Get Minimum value of rating-count list\n",
    "print(\"Min. number of ratings by user:\", numberOfRaitingsRDD.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e888418",
   "metadata": {},
   "source": [
    "#### c) Max number of ratings a user has given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a2fff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. number of ratings by user: 924\n"
     ]
    }
   ],
   "source": [
    "# Get Maximum value of rating-count list\n",
    "print(\"Max. number of ratings by user:\", numberOfRaitingsRDD.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363996cb",
   "metadata": {},
   "source": [
    "#### d) Average number of ratings of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b18f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean. number of ratings by user: 4.814538671191726\n"
     ]
    }
   ],
   "source": [
    "# Get mean (average) value of rating-count list\n",
    "print(\"Mean. number of ratings by user:\", numberOfRaitingsRDD.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899a65e",
   "metadata": {},
   "source": [
    "#### e) Standard deviation for ratings of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a5f734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stdev of number of ratings by user: 5.477802292314525\n"
     ]
    }
   ],
   "source": [
    "# Get Stdev value of rating-count list\n",
    "print(\"Stdev of number of ratings by user:\", numberOfRaitingsRDD.stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a402b5",
   "metadata": {},
   "source": [
    "#### f) Min number of ratings an item has received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49cc37d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. number of ratings of paper: 1\n"
     ]
    }
   ],
   "source": [
    "# Create a pair of (paper_id, 1), execute a word count, restructure to flat list of only count value.\n",
    "raitingsPerItemRDD = paperTermsRDD.map(lambda pair: (pair[0], 1)).reduceByKey(lambda x, y: x+y).map(lambda x: x[1])\n",
    "\n",
    "# Get minimum of the flat list of rating-number\n",
    "print(\"Min. number of ratings of paper:\", raitingsPerItemRDD.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b93c9f",
   "metadata": {},
   "source": [
    "#### g) Max number of ratings an item has received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdba1ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max. number of ratings of paper: 9169\n"
     ]
    }
   ],
   "source": [
    "# Get maximum of the flat list of rating-number\n",
    "print(\"Max. number of ratings of paper:\", raitingsPerItemRDD.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7440695a",
   "metadata": {},
   "source": [
    "#### h) Average number of ratings of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80154b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean. number of ratings of paper: 109.90063866015028\n"
     ]
    }
   ],
   "source": [
    "# Get mean (average) of the flat list of rating-number\n",
    "print(\"Mean. number of ratings of paper:\", raitingsPerItemRDD.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d5c78",
   "metadata": {},
   "source": [
    "#### i) Standard deviation for ratings of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d72bee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stdev of number of ratings of paper: 139.0853409861822\n"
     ]
    }
   ],
   "source": [
    "# Get stdev (standard deviation) of the flat list of rating-number\n",
    "print(\"Stdev of number of ratings of paper:\", raitingsPerItemRDD.stdev())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ce292a",
   "metadata": {},
   "source": [
    "## Exercise 1.5\n",
    "#### In contrast to RDDs, DataFrames allow one to handle structured, distributed data in a table-like representation with named and typed columns. DataFrames are therefore applicable in any instance that requires manipulation of structured data. Load the dataset into DataFrames leveraging the structure of the data. Choose a reasonable schema.\n",
    "\n",
    "First we create the schemas, this was relativly easy since they are largly the same as the definition given in readme.txt. Then we just had to load the data into a dataframe with respect to the schema by using the read() function. In preparation for the next exercise we also already modified the data accordingly. First we split the perform a split in the abstract and user_library columns to turn them into arrays of words/ids and then perform and explode on them (this is very similar to the flattening in 1.3). Also we perform a filtering on the abstracts words using the where function and stopwordsBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f0e93ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6\n",
      "+--------------------+------------+\n",
      "|             user_id|user_library|\n",
      "+--------------------+------------+\n",
      "|28d3f81251d94b097...|     3929762|\n",
      "|28d3f81251d94b097...|      503574|\n",
      "|28d3f81251d94b097...|     5819422|\n",
      "|28d3f81251d94b097...|     4238883|\n",
      "|28d3f81251d94b097...|     5788061|\n",
      "|28d3f81251d94b097...|      462949|\n",
      "|28d3f81251d94b097...|      635215|\n",
      "|28d3f81251d94b097...|      635216|\n",
      "|28d3f81251d94b097...|     4810441|\n",
      "|28d3f81251d94b097...|     3481823|\n",
      "|28d3f81251d94b097...|     4165233|\n",
      "|28d3f81251d94b097...|     3366480|\n",
      "|28d3f81251d94b097...|     5984302|\n",
      "|28d3f81251d94b097...|     4238942|\n",
      "|28d3f81251d94b097...|     5490453|\n",
      "|28d3f81251d94b097...|     4636156|\n",
      "|28d3f81251d94b097...|     5996865|\n",
      "|28d3f81251d94b097...|     4194836|\n",
      "|28d3f81251d94b097...|     5828780|\n",
      "|28d3f81251d94b097...|     4450195|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create DataFrame scheme for papers\n",
    "papers_schema = StructType() \\\n",
    "      .add(\"paper_id\",IntegerType(),True) \\\n",
    "      .add(\"type\",StringType(),True) \\\n",
    "      .add(\"journal\",StringType(),True) \\\n",
    "      .add(\"book_title\",StringType(),True) \\\n",
    "      .add(\"series\",StringType(),True) \\\n",
    "      .add(\"publisher\",StringType(),True) \\\n",
    "      .add(\"pages\",IntegerType(),True) \\\n",
    "      .add(\"volume\",IntegerType(),True) \\\n",
    "      .add(\"number\",IntegerType(),True) \\\n",
    "      .add(\"year\",IntegerType(),True) \\\n",
    "      .add(\"month\",IntegerType(),True) \\\n",
    "      .add(\"postedat\",DateType(),True) \\\n",
    "      .add(\"adress\",StringType(),True) \\\n",
    "      .add(\"title\",StringType(),True) \\\n",
    "      .add(\"abstract\",StringType(),True) \\\n",
    "\n",
    "# Load in dataset of papers\n",
    "papers_df = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(papers_schema) \\\n",
    "      .load(\"papers.csv\")\n",
    "\n",
    "# Split abstracts by space and then explode it, so we get paper-word - pairs\n",
    "actualDF = papers_df.withColumn(\"abstract\", split(col(\"abstract\"), \" \"))\n",
    "wordsDF = actualDF.withColumn(\"abstract\", explode(col('abstract')))\n",
    "\n",
    "# remve all words that are part of the stopwords\n",
    "filteredDF = wordsDF.where(~wordsDF[\"abstract\"].isin(stopwordsBC.value))\n",
    "\n",
    "# create DataFrame for user table\n",
    "users_schema = StructType() \\\n",
    "      .add(\"user_id\",StringType(),True) \\\n",
    "      .add(\"user_library\",StringType(),True)\n",
    "\n",
    "# load in dataset of users\n",
    "users_df= spark.read.options(delimiter=\";\").schema(users_schema).csv(\"users_libraries.txt\")\n",
    "\n",
    "# split ratings by the ',' and then explode, so we get user-rating-pairs.\n",
    "splitDF = users_df.withColumn(\"user_library\", split(col(\"user_library\"), \",\"))\n",
    "ratingsDF = splitDF.withColumn(\"user_library\", explode(col(\"user_library\")))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))\n",
    "\n",
    "ratingsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495a20f",
   "metadata": {},
   "source": [
    "## Exercise 1.6 a)\n",
    "Now we perform 1.3 on a dataframe. We take our already prepared data from before and reduce the paper Dataframe to only the relevant data. After this we join the dataframe on the paper_id and a now column with a \"1\" in each row, similar to the mapping in 1.3. We can now perform the classic reducebykey wordcount trough the groupy, agg and sum methods. After this we sort the data by wordcount and use the windowfunction row_number. Since the data is sorted to have the top word in the top row for each user, the row_number is the same as a \"top words rating\" so to speak. After this we just select all rows where top row number is <= 10, modify the data for writing and save everything in a correctly formated text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d5dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9\n"
     ]
    }
   ],
   "source": [
    "# create reduced set with paper-word-pairs\n",
    "reducedDF = filteredDF.select(col(\"paper_id\"), col(\"abstract\"))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2634f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# (inner) join both tables using the paperIds\n",
    "joinedDF = ratingsDF.join(reducedDF, (ratingsDF[\"user_library\"] == reducedDF[\"paper_id\"]), \"inner\")\n",
    "# Add column with all 1 for word counting\n",
    "joinedDF = joinedDF.withColumn(\"count\", lit(1))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c10a4a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "# Reduce/Count by userId-word-pair\n",
    "wordcountDF = joinedDF.groupBy(\"user_id\", \"abstract\").agg(sum(\"count\"))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3811e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1\n"
     ]
    }
   ],
   "source": [
    "# Evaluate rank of word based on count using window function\n",
    "windowSpec  = Window.partitionBy(\"user_id\").orderBy(desc(\"sum(count)\"))\n",
    "rankedDF = wordcountDF.withColumn(\"row_numbber\",row_number().over(windowSpec))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7652ebc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|               words|\n",
      "+--------------------+--------------------+\n",
      "|00095808cdc611fb5...|[errors, text, si...|\n",
      "|000ac87bf9c1623ee...|[consciousness, p...|\n",
      "|001933555c2b77453...|[game, motivation...|\n",
      "|002bf830c228777bc...|[, detection, dar...|\n",
      "|002e030d14bbb8058...|[archive, culture...|\n",
      "|004d7b66452498748...|[visual, recognit...|\n",
      "|007792b2578fc8df9...|[chromosome, bact...|\n",
      "|008213f1de217daf6...|[potential, model...|\n",
      "|0087a177586a5c9eb...|[search, tasks, i...|\n",
      "|008efe9995cd194f8...|[heating, microwa...|\n",
      "|00a0508d589584122...|[learner, learnin...|\n",
      "|00a19396b7bb52e40...|[face, recognitio...|\n",
      "|00b45845b91a446a8...|[teacher, profess...|\n",
      "|00bc1bb009b21847c...|[strategic, {erp}...|\n",
      "|00cec95a2c007b650...|[force, states, s...|\n",
      "|00cfce7a36abfc91e...|[10.1093/hrlr/4.1.1]|\n",
      "|00d08b45f5ce47630...|[gene, data, tree...|\n",
      "|00d145bb7325f62e5...|[graphics, proces...|\n",
      "|01071eb603bbdb648...|[hydroxyurea, cel...|\n",
      "|01467ff62bfe31efe...|[protein, structu...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "118.6\n"
     ]
    }
   ],
   "source": [
    "# Create tempView, so we can apply sql queries\n",
    "rankedDF.createOrReplaceTempView(\"rankedDF\");\n",
    "\n",
    "# only get the top 10 words of every user in descending order\n",
    "finalDF = spark.sql(\"SELECT user_id, abstract, row_numbber FROM rankedDF WHERE row_numbber <= 10\")\n",
    "\n",
    "# Combine single words to one string\n",
    "finalDF = finalDF.groupBy('user_id').agg(collect_list('abstract').alias('words'))\n",
    "finalDF.show()\n",
    "\n",
    "# Put everything in just one cell, so we can easily print it\n",
    "dataFrameWithOnlyOneColumn = finalDF.select(concat_ws(\",\", col(\"user_id\"), col('words')).alias('data'))\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a859b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.1\n"
     ]
    }
   ],
   "source": [
    "# Output table to textfile withoud headers.\n",
    "dataFrameWithOnlyOneColumn.coalesce(1).write.format(\"text\").option(\"header\", \"false\").save(\"wordcountDF\")\n",
    "\n",
    "run_time = (time.time() - start_time)\n",
    "print(\"%.1f\" % (run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cee14a",
   "metadata": {},
   "source": [
    "## Exercise 1.6 b\n",
    "\n",
    "This is effectivly just the same as in 1.4, the comments should be enough of an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf7da553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT user_id)|\n",
      "+-----------------------+\n",
      "|                  28416|\n",
      "+-----------------------+\n",
      "\n",
      "+------------------------+\n",
      "|count(DISTINCT paper_id)|\n",
      "+------------------------+\n",
      "|                  172079|\n",
      "+------------------------+\n",
      "\n",
      "+-------------------+\n",
      "|count(user_library)|\n",
      "+-------------------+\n",
      "|             828481|\n",
      "+-------------------+\n",
      "\n",
      "+--------------+\n",
      "|min(wordCount)|\n",
      "+--------------+\n",
      "|             1|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|max(wordCount)|\n",
      "+--------------+\n",
      "|          1922|\n",
      "+--------------+\n",
      "\n",
      "+------------------+\n",
      "|    avg(wordCount)|\n",
      "+------------------+\n",
      "|29.155440596846848|\n",
      "+------------------+\n",
      "\n",
      "+----------------------+\n",
      "|stddev_samp(wordCount)|\n",
      "+----------------------+\n",
      "|     81.17660451011598|\n",
      "+----------------------+\n",
      "\n",
      "+----------+\n",
      "|min(count)|\n",
      "+----------+\n",
      "|         3|\n",
      "+----------+\n",
      "\n",
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|       924|\n",
      "+----------+\n",
      "\n",
      "+----------------+\n",
      "|      avg(count)|\n",
      "+----------------+\n",
      "|4.81453867119172|\n",
      "+----------------+\n",
      "\n",
      "+------------------+\n",
      "|stddev_samp(count)|\n",
      "+------------------+\n",
      "| 5.477818208917269|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.6 b)\n",
    "\n",
    "#a) Number of (distinct) user, number of (distinct) items, and number of ratings\n",
    "users_df.select(countDistinct(\"user_id\")).show()\n",
    "papers_df.select(countDistinct(\"paper_id\")).show()\n",
    "ratingsDF.select(count(\"user_library\")).show()\n",
    "\n",
    "#b) Min number of ratings a user has given\n",
    "users_df.withColumn('wordCount', size(split(col('user_library'), ','))).select(min(col(\"wordCount\"))).show()\n",
    "\n",
    "#c) Max number of ratings a user has given\n",
    "users_df.withColumn('wordCount', size(split(col('user_library'), ','))).select(max(col(\"wordCount\"))).show()\n",
    "\n",
    "#d) Average number of ratings of users\n",
    "users_df.withColumn('wordCount', size(split(col('user_library'), ','))).select(avg(col(\"wordCount\"))).show()\n",
    "\n",
    "#e) Standard deviation for ratings of users\n",
    "users_df.withColumn('wordCount', size(split(col('user_library'), ','))).select(stddev(col(\"wordCount\"))).show()\n",
    "\n",
    "#f) Min number of ratings an item has received\n",
    "ratingsDF.groupBy(\"user_library\").count().select(min(col(\"count\"))).show()\n",
    "\n",
    "#g) Max number of ratings an item has received\n",
    "ratingsDF.groupBy(\"user_library\").count().select(max(col(\"count\"))).show()\n",
    "\n",
    "#h) Average number of ratings of items\n",
    "ratingsDF.groupBy(\"user_library\").count().select(avg(col(\"count\"))).show()\n",
    "\n",
    "#i) Standard deviation for ratings of items\n",
    "ratingsDF.groupBy(\"user_library\").count().select(stddev(col(\"count\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b4ef6",
   "metadata": {},
   "source": [
    "#### Runtime comparison\n",
    "\n",
    "It took 991 seconds to do the evaluation using RDD.\n",
    "DF were significantly faster (factor of nearly 4) with 255.1 seconds.\n",
    "\n",
    "Since a lot of work that is done in RDDs (map/flatMap/reduce/...) takes python code as a parameter it will run this code on every occasion where data has to be compared. Therefore this is not native code but slow pyhton stuff and it is not optimized, causing long runtimes.  \n",
    "Additionally DataFrames get compiled for execution while RDDs are code that is running directly in python, giving DataFrames a generally faster execution speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023b335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
